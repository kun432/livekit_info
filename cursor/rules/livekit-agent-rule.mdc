---
description: This rule help cursor with the update LiveKit Agents 1.0
globs: *.py
alwaysApply: false
---
@livekit-rule.mdc

# LiveKit Agent 1.0 Rules

You are an expert at Python devlopment and use best practices and principles when write code. You are also an expert at creating LiveKit agents to produce well organized and structured code to enable real-time interactive multi agent (Agent) solutions. You will re-engineer existing code that used pre 1.0 library into well structured 1.0 compatible code.


## Agent 1.0 is pre-release

Currently Agent 1.0 is in development. Until release `requirements.txt` should install from the GitHub repo like the following:


**requirements.txt**

```
livekit

# Agent DEV
git+https://github.com/livekit/agents.git@dev-1.0#subdirectory=livekit-agents


# Plugins DEV
git+https://github.com/livekit/agents.git@dev-1.0#subdirectory=livekit-plugins/livekit-plugins-openai
git+https://github.com/livekit/agents.git@dev-1.0#subdirectory=livekit-plugins//livekit-plugins-deepgram
git+https://github.com/livekit/agents.git@dev-1.0#subdirectory=livekit-plugins/livekit-plugins-cartesia
git+https://github.com/livekit/agents.git@dev-1.0#subdirectory=livekit-plugins/livekit-plugins-elevenlabs

etc...
```


## Key Changes

This is a a summary Agent >=1.0 key changes from previous versions. If any of the "old" packages or class names appear in the current file Cursor should offer code to migrate the code.

* `VoicePipelineAgent` and `MultimodalAgent` are now combined into a single interface `AgentSession`
    * Model will be used to determine if 
* `AssistantLLM` has been removed and should be converted to regular `AgentSession` with a similar model that does not use the Assistant API
* `STTSegmentsForwarder` has been removed
* The package `ivekit.agents.pipeline` has been renamed to `livekit.agents.voice`
* `AgentSession` can use `instruction` or an `Agent` but generally `Agent` should be prefferd now
* `@agent.on` no longer exists. Now override `Agent` to receive hooks like [`on_enter`, `on_exit`, and `on_end_of_turn`]
* LLM, STT, TTS, and VAD


## Minimal Voice Agent

The following is a common structure of a `AgentSession`. It is also common to break out `Agent`s into their own source file to keep code more organized and structure.

```
import logging

from dotenv import load_dotenv
from livekit.agents import JobContext, WorkerOptions, cli
from livekit.agents.llm import function_tool
from livekit.agents.voice import Agent, RunContext, AgentSession, UserStartedSpeakingEvent
from livekit.agents.voice.room_io import RoomInputOptions
from livekit.plugins import cartesia, deepgram, openai

# from livekit.plugins import noise_cancellation

logger = logging.getLogger("roomio-example")
logger.setLevel(logging.INFO)

load_dotenv()


class EchoAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="You are Echo.",
            # llm=openai.realtime.RealtimeModel(voice="echo"),
            stt=deepgram.STT(),
            llm=openai.LLM(model="gpt-4o-mini"),
            tts=cartesia.TTS(),
        )

    def on_enter(self):
        # Register event handler when task becomes active
        self.agent.on("user_started_speaking", self.handle_speech_start)
        
    def handle_speech_start(self, ev: UserStartedSpeakingEvent):
        print("User started speaking - activating noise cancellation")
        # Implement your custom logic here
        # Example: self.agent.output.audio.enable_noise_cancellation(True)

    @function_tool
    async def talk_to_alloy(self, context: RunContext):
        return AlloyAgent(), "Transferring you to Alloy."


class AlloyAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="You are Alloy.",
            llm=openai.realtime.RealtimeModel(voice="alloy"),
        )

    @function_tool
    async def talk_to_echo(self, context: RunContext):
        return EchoAgent(), "Transferring you to Echo."


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    session = AgentSession()

    await session.start(
        agent=AlloyAgent()
        room=ctx.room,
        room_input_options=RoomInputOptions(
            # noise_cancellation=noise_cancellation.BVC(),
        ),
    )


if __name__ == "__main__":
    cli.run_app(WorkerOptions(
        entrypoint_fnc=entrypoint
        # agent_name="smart-agent"
    ))

```


## Example Migration


### Previous Agent Example

```
import logging

from dotenv import load_dotenv
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli, llm
from livekit.agents.pipeline import VoicePipelineAgent
from livekit.plugins import deepgram, openai, silero
from livekit.plugins.openai.beta import (
    AssistantCreateOptions,
    AssistantLLM,
    AssistantOptions,
    OnFileUploadedInfo,
)

load_dotenv()
logger = logging.getLogger("openai_assistant")


async def entrypoint(ctx: JobContext):
    """This example demonstrates a VoicePipelineAgent that uses OpenAI's Assistant API as the LLM"""
    initial_ctx = llm.ChatContext()

    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    participant = await ctx.wait_for_participant()

    # When you add a ChatMessage that contain images, AssistantLLM will upload them
    # to OpenAI's Assistant API.
    # It's up to you to remove them if desired or otherwise manage them going forward.
    def on_file_uploaded(info: OnFileUploadedInfo):
        logger.info(f"{info.type} uploaded: {info.openai_file_object}")

    agent = VoicePipelineAgent(
        vad=silero.VAD.load(),
        stt=deepgram.STT(),
        llm=AssistantLLM(
            assistant_opts=AssistantOptions(
                create_options=AssistantCreateOptions(
                    model="gpt-4o",
                    instructions="You are a voice assistant created by LiveKit. Your interface with users will be voice.",
                    name="KITT",
                )
            ),
            on_file_uploaded=on_file_uploaded,
        ),
        tts=openai.TTS(),
        chat_ctx=initial_ctx,
    )
    agent.start(ctx.room, participant)
    await agent.say("Hey, how can I help you today?", allow_interruptions=False)


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))

```

### New Agent >= 1.0 Example

```
import logging

from dotenv import load_dotenv
from livekit.agents import JobContext, WorkerOptions, cli
from livekit.agents.voice import Agent, AgentSession
from livekit.agents.voice.room_io import RoomInputOptions
from livekit.plugins import deepgram, openai, silero

load_dotenv()
logger = logging.getLogger("openai_assistant")


class AlloyAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="You are Echo.",
            stt=openai.STT(),
            llm=openai.LLM(model="gpt-4o-mini"),
            tts=openai.TTS(),
            vad=silero.VAD.load(),
        )


async def entrypoint(ctx: JobContext):
    """AgentSession using OpenAI's GPT-4o model"""
    await ctx.connect()

    session = AgentSession()

    await session.start(
        agent=AlloyAgent(),
        room=ctx.room,
        room_input_options=RoomInputOptions(),
    )


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```


## Plugins

LiveKit agents depends on plugins to provide services for STT, TTS, LLM, End-of-Turn, and more. 

Example of libraries that can be loaded if the agent code depends on a given plugin.

* Anthropic[LLM]: [livkiet-plugins-anthropic.mdc](mdc:.cursor/rules/livkiet-plugins-anthropic.mdc)
* Assemblyai: [livkiet-plugins-assemblyai.mdc](mdc:.cursor/rules/livkiet-plugins-assemblyai.mdc)
* AWS[LLM, STT, TTS]: [livkiet-plugins-aws.mdc](mdc:.cursor/rules/livkiet-plugins-aws.mdc)
* Azure[STT, TTS]: [livkiet-plugins-azure.mdc](mdc:.cursor/rules/livkiet-plugins-azure.mdc)  (Azure LLM is found in OpenAI plugin)
* Cartesia[TTS]: [livkiet-plugins-cartesia.mdc](mdc:.cursor/rules/livkiet-plugins-cartesia.mdc)
* Clova[STT]: [livkiet-plugins-clova.mdc](mdc:.cursor/rules/livkiet-plugins-clova.mdc)
* Deepgram[TTS, STT]: [livekit-plugins-deepgram.mdc](mdc:.cursor/rules/livekit-plugins-deepgram.mdc)
* Elevenlabs[TTS]: [livkiet-plugins-elevenlabs.mdc](mdc:.cursor/rules/livkiet-plugins-elevenlabs.mdc)
* FAL[STT]: [livkiet-plugins-fal.mdc](mdc:.cursor/rules/livkiet-plugins-fal.mdc)
* Google[LLM, STT, TTS]: [livkiet-plugins-google.mdc](mdc:.cursor/rules/livkiet-plugins-google.mdc)
* LamaIndex[LLM]: [livkiet-plugins-llama-index.mdc](mdc:.cursor/rules/livkiet-plugins-llama-index.mdc)
* NLTK: [livkiet-plugins-nltk.mdc](mdc:.cursor/rules/livkiet-plugins-nltk.mdc)
* OpenAI[STT, TTS, LLM]: [livekit-plugins-openai.mdc](mdc:.cursor/rules/livekit-plugins-openai.mdc)
* PlayAI[TTS]: [livkiet-plugins-playai.mdc](mdc:.cursor/rules/livkiet-plugins-playai.mdc)
* RAG: [livkiet-plugins-rag.mdc](mdc:.cursor/rules/livkiet-plugins-rag.mdc)
* Rime[TTS]: [livkiet-plugins-rime.mdc](mdc:.cursor/rules/livkiet-plugins-rime.mdc)
* Silero[VAD]: [livkiet-plugins-silero.mdc](mdc:.cursor/rules/livkiet-plugins-silero.mdc)
* Speechmatcis[STT]: [livkiet-plugins-speechmatics.mdc](mdc:.cursor/rules/livkiet-plugins-speechmatics.mdc)
* TurnDetector (EOT): [livkiet-plugins-turn-detector.mdc](mdc:.cursor/rules/livkiet-plugins-turn-detector.mdc)



